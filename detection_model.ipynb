{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "detection_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TedDeVriesLentsch/ComputerVision_Group20/blob/main/detection_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUwMxr2GSaRB"
      },
      "source": [
        "# Project - Seminar Computer Vision by Deep Learning (CS4245) 2020/2021\n",
        "\n",
        "Group Number: 20\n",
        "\n",
        "Student 1: Stan Zwinkels\n",
        "\n",
        "Student 2: Ted de Vries Lentsch\n",
        "\n",
        "Date: June 14, 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjb3bsZI49TG"
      },
      "source": [
        "## Check available GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7IeSE3K49TH"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyryToom6ESk"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssOGw6Io6GBi"
      },
      "source": [
        "DO_TRAIN                = False\n",
        "DO_TRAIN_NOISE          = False\n",
        "DO_TRAIN_COLORJITTER    = False\n",
        "DO_TEST                 = False\n",
        "DO_OPTIMIZE_THRESHOLD   = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAPVQ8tW0jI-"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIVdMiWT0jI-",
        "scrolled": true
      },
      "source": [
        "# standard libraries\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# data processing\n",
        "from skimage.transform import resize\n",
        "\n",
        "# widgets\n",
        "import ipywidgets\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.transforms import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM72kG3250tR"
      },
      "source": [
        "## Download and import standard TorchVision files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb-0HR_r5wcr"
      },
      "source": [
        "%%shell\n",
        "# install pycocotools\n",
        "pip install -q -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "\n",
        "# clone TorchVision repository to use some files from references/detection\n",
        "git clone --quiet https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout --quiet v0.3.0\n",
        "\n",
        "# copy the files to drive\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL6GoLT91TB"
      },
      "source": [
        "from engine import train_one_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNXfZUAO0jJA"
      },
      "source": [
        "## Determine device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu9kvyP_0jJB",
        "scrolled": true
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('The device is: {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gTuUXf-21q0"
      },
      "source": [
        "## Load dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O63BH4t26JB"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We4624BL3Z1H"
      },
      "source": [
        "Go to your Kaggle account, scroll to the `API` section and click `Expire API Token` to remove previous tokens. Then click on `Create New API Token` and the file `kaggle.json` is downloaded. Run the cell below and select the `kaggle.json` that has been downloaded from the Kaggle account settings page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJGNK0a03Thh"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB8G4djO7ELN"
      },
      "source": [
        "Place the `kaggle.json` file in the directory `~/.kaggle/kaggle.json` according to the API of Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt1ymYVa4h7W"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFMWPkHC5f7I"
      },
      "source": [
        "Download the public dataset from Kaggle and unzip the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLy3Ni9P4tFO"
      },
      "source": [
        "!kaggle datasets download -d teddevrieslentsch/morado-5may\n",
        "!unzip -qq morado-5may.zip -d morado_5may\n",
        "!rm morado-5may.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJo2WONZ3_A3"
      },
      "source": [
        "## Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUY-I36A3-O4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QJeafBR5cqf"
      },
      "source": [
        "Create folder on your Drive to save model parameters after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiJKux8W4WmK"
      },
      "source": [
        "if not os.path.isdir('drive/MyDrive/model_parameters'):\n",
        "    os.makedirs('drive/MyDrive/model_parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esdpweAs0jJD"
      },
      "source": [
        "## 1. Explore dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy6I5Iwu0jJD"
      },
      "source": [
        "### 1.1. Load data\n",
        "Below, the names of the images and annotations are obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k0_GVoM0jJE",
        "scrolled": true
      },
      "source": [
        "# paths\n",
        "dataset_name  = 'morado_5may'\n",
        "path_img      = '{}/images'.format(dataset_name)\n",
        "path_annot    = '{}/annotations'.format(dataset_name)\n",
        "\n",
        "# get image and annotation names\n",
        "img_names     = list(sorted(os.listdir(path_img)))\n",
        "annot_names   = list(sorted(os.listdir(path_annot)))\n",
        "\n",
        "print('The dataset has {} images and {} annotations.'.format(len(img_names), len(annot_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1e7LEZ349TM"
      },
      "source": [
        "### 1.2. Images\n",
        "Below, the images are displayed with an interactive widget. The size of every image is (H, W, C) = (4032, 3024, 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLyYk3-b49TM"
      },
      "source": [
        "def plot_img(path_img, img_name):\n",
        "    I1 = plt.imread('{}/{}'.format(path_img, img_name))\n",
        "    I2 = np.rot90(I1, -1)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(I2, zorder=-10)\n",
        "    plt.xlabel('Width of image (pixels)', fontsize=20)\n",
        "    plt.ylabel('Height of image (pixels)', fontsize=20)\n",
        "    plt.xticks(np.linspace(0, 3024, 9), fontsize=16)\n",
        "    plt.yticks(np.linspace(0, 4032, 9), fontsize=16)\n",
        "    plt.xlim(0, 3024)\n",
        "    plt.ylim(4032, 0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOz7T1C-_RrF"
      },
      "source": [
        "ipywidgets.interact(lambda idx: plot_img(path_img, img_names[idx]), idx=range(len(img_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gMujMpz49TM"
      },
      "source": [
        "### 1.3. Annotations\n",
        "Below, the images and annotations from the dataset are displayed with an interactive widget. The size of every image is (H, W, C) = (4032, 3024, 3) and an array with annotations constains rows with [x_min, y_min, x_max, y_max, label]. The 'raw' flowers are indicated with a blue rectangle and the 'ripe' flowers with a red rectangle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYGAaJx949TN"
      },
      "source": [
        "def plot_img_and_annots(path_img, img_name, path_annot, annot_name):\n",
        "    I1      = plt.imread('{}/{}'.format(path_img, img_name))\n",
        "    I2      = np.rot90(I1, -1)\n",
        "    annot1  = pd.read_csv('{}/{}'.format(path_annot, annot_name), sep=',', header=None)\n",
        "    annot2  = annot1.replace({'raw': 1, 'ripe': 2}).to_numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(I2, zorder=-10)\n",
        "    for coor in annot2:\n",
        "        rectangle_points = np.array([[coor[0], coor[2], coor[2], coor[0], coor[0]],\n",
        "                                     [coor[1], coor[1], coor[3], coor[3], coor[1]]])\n",
        "        if coor[4]==1:\n",
        "            plt.plot(rectangle_points[0,:], rectangle_points[1,:], color='blue', linewidth=3, zorder=10)\n",
        "        elif coor[4]==2:\n",
        "            plt.plot(rectangle_points[0,:], rectangle_points[1,:], color='red', linewidth=3, zorder=10)\n",
        "    plt.xlabel('Width of image (pixels)', fontsize=20)\n",
        "    plt.ylabel('Height of image (pixels)', fontsize=20)\n",
        "    plt.xticks(np.linspace(0, 3024, 9), fontsize=16)\n",
        "    plt.yticks(np.linspace(0, 4032, 9), fontsize=16)\n",
        "    plt.xlim(0, 3024)\n",
        "    plt.ylim(4032, 0)\n",
        "    line_raw  = Line2D([0], [0], color='blue', linewidth=3, label='raw')\n",
        "    line_ripe = Line2D([0], [0], color='red', linewidth=3, label='ripe')\n",
        "    plt.legend(handles=[line_raw, line_ripe], loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20) \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBH0uPDT_F1n"
      },
      "source": [
        "ipywidgets.interact(lambda idx: plot_img_and_annots(path_img, img_names[idx], path_annot, annot_names[idx]), idx=range(len(img_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHohuKKP49TN"
      },
      "source": [
        "### 1.4. Resized image\n",
        "Below, the images and annotations from the dataset are displayed with an interactive widget. The image needs to be resized because the height and width of an input image for Faster R-CNN must be in the range [800, 1333]. The original images have a size of (H, W, C) = (4032, 3024, 3). We have chosen to resize the image so that the height to width ratio remains the same. The size of each resized image is (H, W, C) = (1200, 900, 3) and an array with annotations constains rows with [x_min, y_min, x_max, y_max, label]. The 'raw' flowers are indicated with a blue rectangle and the 'ripe' flowers with a red rectangle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaxOVrnx49TN"
      },
      "source": [
        "def plot_img_and_annots_resized(path_img, img_name, path_annot, annot_name):\n",
        "    I1      = plt.imread('{}/{}'.format(path_img, img_name))\n",
        "    I2      = np.rot90(I1, -1)\n",
        "    I3      = resize(I2, (1200, 900))\n",
        "    annot1  = pd.read_csv('{}/{}'.format(path_annot, annot_name), sep=',', header=None)\n",
        "    annot2  = annot1.replace({'raw': 1, 'ripe': 2}).to_numpy()\n",
        "    annot2[:,0:4]  = (1200/4032)*annot2[:,0:4]\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(I3, zorder=-10)\n",
        "    for coor in annot2:\n",
        "        rectangle_points = np.array([[coor[0], coor[2], coor[2], coor[0], coor[0]],\n",
        "                                     [coor[1], coor[1], coor[3], coor[3], coor[1]]])\n",
        "        if coor[4]==1:\n",
        "            plt.plot(rectangle_points[0,:], rectangle_points[1,:], color='blue', linewidth=3, zorder=10)\n",
        "        elif coor[4]==2:\n",
        "            plt.plot(rectangle_points[0,:], rectangle_points[1,:], color='red', linewidth=3, zorder=10)\n",
        "    plt.xlabel('Width of image (pixels)', fontsize=20)\n",
        "    plt.ylabel('Height of image (pixels)', fontsize=20)\n",
        "    plt.xticks(np.linspace(0, 900, 7), fontsize=16)\n",
        "    plt.yticks(np.linspace(0, 1200, 9), fontsize=16)\n",
        "    plt.xlim(0, 900)\n",
        "    plt.ylim(1200, 0)\n",
        "    line_raw  = Line2D([0], [0], color='blue', linewidth=3, label='raw')\n",
        "    line_ripe = Line2D([0], [0], color='red', linewidth=3, label='ripe')\n",
        "    plt.legend(handles=[line_raw, line_ripe], loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20)    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czcVJ6f0_L4g"
      },
      "source": [
        "ipywidgets.interact(lambda idx: plot_img_and_annots_resized(path_img, img_names[idx], path_annot, annot_names[idx]), idx=range(len(img_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz9iDgSF49TO"
      },
      "source": [
        "## 2. Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "turanSC-49TO"
      },
      "source": [
        "class MoradoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, path_numbers=[], transforms=None):\n",
        "        self.root       = root                                                          # directory to dataset\n",
        "        self.transforms = transforms                                                    # transform input data\n",
        "        self.imgs       = self.get_names('{}/images'.format(root), path_numbers)        # load images\n",
        "        self.annots     = self.get_names('{}/annotations'.format(root), path_numbers)   # load annotations\n",
        "        self.classes    = ['background', 'raw', 'ripe']                                 # classes\n",
        "        self.height     = 1200                                                          # (in pixels)\n",
        "        self.width      = 900                                                           # (in pixels)\n",
        "        self.sc_factor  = 1200/4032                                                     # scale factor\n",
        "\n",
        "    def get_names(self, directory, path_numbers):\n",
        "        names         = list(sorted(os.listdir(directory)))\n",
        "        target_names  = []\n",
        "\n",
        "        for name in names:\n",
        "            path_number = int(name.split('morado_5may_')[1].split('_')[0])\n",
        "            if path_number in path_numbers:\n",
        "                target_names.append(name)\n",
        "        \n",
        "        return target_names#[:4] #REMOVE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path   = '{}/images/{}'.format(self.root, self.imgs[idx])                   # image path\n",
        "        annot_path = '{}/annotations/{}'.format(self.root, self.annots[idx])            # annotation path\n",
        "\n",
        "        img   = np.rot90(plt.imread(img_path), -1)                                                          # image (rotated)\n",
        "        annot = pd.read_csv(annot_path, sep=',', header=None).replace({'raw': 1, 'ripe': 2}).to_numpy()     # annotation\n",
        "\n",
        "        img          = resize(img, (self.height, self.width))                           # resized image\n",
        "        annot[:,0:4] = self.sc_factor*annot[:,0:4]                                      # resized annotation\n",
        "\n",
        "        boxes    = torch.as_tensor(annot[:,0:4], dtype=torch.float)                     # boxes\n",
        "        labels   = torch.as_tensor(annot[:,4], dtype=torch.int64)                       # labels\n",
        "        image_id = torch.tensor([idx])                                                  # image id\n",
        "        area     = (boxes[:,3]-boxes[:,1])*(boxes[:,2]-boxes[:,0])                      # area\n",
        "        iscrowd  = torch.zeros((len(annot)), dtype=torch.int64)                         # is crowd (set to False)\n",
        "\n",
        "        target             = {}                                                         # target\n",
        "        target['boxes']    = boxes\n",
        "        target['labels']   = labels\n",
        "        target['image_id'] = image_id\n",
        "        target['area']     = area\n",
        "        target['iscrowd']  = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naqPzSNN49TP"
      },
      "source": [
        "## 3. Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQN1VCk749TP"
      },
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, img, target):\n",
        "        img = F.to_tensor(img.copy()).type(torch.float)\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsJOJdfN49TP"
      },
      "source": [
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, prob_threshold):\n",
        "        self.prob_threshold = prob_threshold\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random()<self.prob_threshold:\n",
        "            width           = img.shape[2]                  # get width of image\n",
        "            img             = img.flip(-1)                  # mirror image horizontally\n",
        "            bbox            = target['boxes']               # get bounding boxes of original image\n",
        "            bbox[:,[0,2]]   = width-bbox[:,[2,0]]           # adapt x-coordinates of bounding boxes\n",
        "            target['boxes'] = bbox                          # set new bounding box as target\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p853062IJZNi"
      },
      "source": [
        "class RandomVerticalFlip(object):\n",
        "    def __init__(self, prob_threshold):\n",
        "        self.prob_threshold = prob_threshold\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        if random.random()<self.prob_threshold:\n",
        "            height          = img.shape[1]                  # get height of image\n",
        "            img             = img.flip([1])                 # mirror image vertically\n",
        "            bbox            = target['boxes']               # get bounding boxes of original image\n",
        "            bbox[:,[1,3]]   = height-bbox[:,[3,1]]          # adapt y-coordinates of bounding boxes\n",
        "            target['boxes'] = bbox                          # set new bounding box as target\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm2u0IA2NkRo"
      },
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0, std=0.3):\n",
        "        self.mean = mean\n",
        "        self.std  = std\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        # torch.randn gives tensor filled with random numbers from the standard normal distribution\n",
        "        img = torch.clamp(img + self.mean + torch.randn(img.size())*self.std, min=0, max=1)\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9mzJaKqg6Mi"
      },
      "source": [
        "class ColorJitter(object):\n",
        "    def __call__(self, img, target):\n",
        "        color_jitter = torchvision.transforms.ColorJitter(brightness=0, contrast=0.3, saturation=0.3, hue=0.2)\n",
        "        img = color_jitter(img)\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7PgMuUW49TP"
      },
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, target):\n",
        "        for transform in self.transforms:\n",
        "            img, target = transform(img, target)\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFUOIxVg49TQ"
      },
      "source": [
        "# Note: mean/std normalization is done by model\n",
        "def get_transform(train=False, train_noise=False, train_colorjitter=False):\n",
        "    transforms = []\n",
        "    \n",
        "    # numpy array to tensor and 0-255 to 0-1\n",
        "    transforms.append(ToTensor())\n",
        "    \n",
        "    if train:\n",
        "        # during training, randomly flip the training images and bounding boxes for data augmentation\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "        transforms.append(RandomVerticalFlip(0.5))\n",
        "\n",
        "        if train_noise:\n",
        "            # During training add GaussianNoise on the training images\n",
        "            transforms.append(AddGaussianNoise())\n",
        "\n",
        "        if train_colorjitter:\n",
        "            # During training add ColorJitter on the training images (brightness, constrast, saturation, hue)\n",
        "            transforms.append(ColorJitter())\n",
        "    return Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-pGbNP49TQ"
      },
      "source": [
        "## 4. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ZyeXAn49TQ"
      },
      "source": [
        "def get_model(num_classes):\n",
        "    # load Faster R-CNN model (pre-trained on COCO)\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # there are 3 different classes (background + 2 classes)\n",
        "    num_classes = 3\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # replace the classifier with one that has num_classes classes\n",
        "    # head: part of the network that uses ROI feature vector to predict cls score and bounding box (2 sibling linear layers)\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T04U9RiP49TQ"
      },
      "source": [
        "## 5. Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4fzYrPQ49TR"
      },
      "source": [
        "# set seed for repetitiveness\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# path numbers for the datasets (there are 10 paths)\n",
        "path_numbers_train  = [1, 2, 3, 4, 5, 6]\n",
        "path_numbers_val    = [7, 8]\n",
        "path_numbers_test   = [9, 10]\n",
        "\n",
        "# create train, validation, and test dataset\n",
        "dataset_train   = MoradoDataset('morado_5may', path_numbers_train, get_transform(DO_TRAIN, DO_TRAIN_NOISE, DO_TRAIN_COLORJITTER))\n",
        "dataset_val     = MoradoDataset('morado_5may', path_numbers_val, get_transform())\n",
        "dataset_test    = MoradoDataset('morado_5may', path_numbers_test, get_transform())\n",
        "\n",
        "# utility function for data loader\n",
        "# convert batch from [(img1, dict1), (img2, dict2)] into ((img1, img2), (dict1, dict2))\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# create train data loaders and test \n",
        "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=2, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
        "data_loader_val   = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
        "data_loader_test  = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=1, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI4c2Jhj90hI"
      },
      "source": [
        "## 6. Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3UIPvg549TR"
      },
      "source": [
        "# there are 3 different classes (background + 2 classes)\n",
        "num_classes = 3\n",
        "\n",
        "# get the model\n",
        "model = get_model(num_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FSd9uUg49TR"
      },
      "source": [
        "## 7. Train and save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjNEifiwh7Tl"
      },
      "source": [
        "def evaluate_model(model, dataset_val, device):\n",
        "    # variable to store the loss\n",
        "    total_loss = 0\n",
        "\n",
        "    # keep model in train mode to obtain loss dictionary\n",
        "    model.train()\n",
        "\n",
        "    # disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for img, target in dataset_val:\n",
        "            img     = img.to(device)\n",
        "            target  = {k: v.to(device) for k, v in target.items()}\n",
        "\n",
        "            # predict\n",
        "            loss_dict = model([img], [target])\n",
        "            loss      = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            # add loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    # average loss\n",
        "    avg_loss = total_loss/len(dataset_val)\n",
        "\n",
        "    return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCSrhRqM8-Sd"
      },
      "source": [
        "if DO_TRAIN:\n",
        "    # start time training is used for name of saved model  \n",
        "    saving_time = int(time.time())\n",
        "\n",
        "    # training variables\n",
        "    params          = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    NUM_EPOCHS      = 15\n",
        "    LEARNING_RATE   = 0.005\n",
        "    MOMENTUM        = 0.9\n",
        "    WEIGHT_DECAY    = 0.0005\n",
        "    STEP_SIZE       = 3\n",
        "    GAMMA           = 0.1\n",
        "\n",
        "    optimizer       = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    lr_scheduler    = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "    # early stopping implementation\n",
        "    best_val_loss = float('inf')\n",
        "    patience      = 3\n",
        "    patience_cnt  = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # train for one epoch\n",
        "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=30)\n",
        "        \n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # evaluate on the validation dataset\n",
        "        val_loss = evaluate_model(model, dataset_val, device)\n",
        "        print('The validation loss is {:.4f}'.format(val_loss))\n",
        "        print()\n",
        "\n",
        "        # apply early stopping\n",
        "        if val_loss<best_val_loss:\n",
        "            patience_cnt = 0\n",
        "            best_val_loss = val_loss\n",
        "            \n",
        "            # save model\n",
        "            torch.save(model.state_dict(), 'drive/MyDrive/model_parameters/model_{}.pt'.format(saving_time))\n",
        "\n",
        "            # save train info\n",
        "            columns           = ['LOSS','EPOCH','NUM_EPOCHS','LEARNING_RATE','MOMENTUM','WEIGHT_DECAY','STEP_SIZE','GAMMA','TRANSFORMS']\n",
        "            train_variables   = np.array([[val_loss, epoch+1, NUM_EPOCHS, LEARNING_RATE, MOMENTUM, WEIGHT_DECAY, STEP_SIZE, GAMMA, 0]])\n",
        "            transform_names   = [obj.__class__.__name__ for obj in get_transform(train=True).transforms]\n",
        "            model_info        = pd.DataFrame(train_variables, columns=columns).astype(object)\n",
        "            model_info.at[0,'TRANSFORMS'] = transform_names\n",
        "            model_info.to_csv('drive/MyDrive/model_parameters/model_info_{}.csv'.format(saving_time), index=0, header=1)\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "\n",
        "            if patience_cnt==patience:              \n",
        "                # stop training\n",
        "                break\n",
        "\n",
        "    print('The model has been saved as {}'.format('drive/MyDrive/model_parameters/model_{}.pt'.format(saving_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTkJ-Os7F3J2"
      },
      "source": [
        "## 8. Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V57IpxKyF7AN"
      },
      "source": [
        "if DO_TEST:\n",
        "    if not DO_TRAIN:\n",
        "        saving_time = None\n",
        "        model.load_state_dict(torch.load('drive/MyDrive/model_parameters/model_{}.pt'.format(saving_time)))\n",
        "    else:\n",
        "        model.load_state_dict(torch.load('drive/MyDrive/model_parameters/model_{}.pt'.format(saving_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DbArk6B0Po4"
      },
      "source": [
        "## 9. Test model and show predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzuzuDOdXxOR"
      },
      "source": [
        "def do_nms(boxes, labels, scores, IoU_threshold=0.3):\n",
        "    # get indices to keep\n",
        "    keep        = torchvision.ops.nms(boxes, scores, IoU_threshold)\n",
        "\n",
        "    # keep selection\n",
        "    boxes_nms   = boxes[keep]\n",
        "    labels_nms  = labels[keep]\n",
        "    scores_nms  = scores[keep]\n",
        "\n",
        "    return boxes_nms, labels_nms, scores_nms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyjf3T3-Fyoa"
      },
      "source": [
        "def test_model(model, device, dataset):\n",
        "    # disable gradients\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # switch model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # empty lists for the ground truth and predicted arrays\n",
        "    Is           = []\n",
        "    boxes_gts    = []\n",
        "    labels_gts   = []\n",
        "    boxes_preds  = []\n",
        "    labels_preds = []\n",
        "    scores_preds = []\n",
        "\n",
        "    # evaluate test dataset\n",
        "    with torch.no_grad():\n",
        "        for img, target in dataset:\n",
        "            # predict\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "            # append image\n",
        "            Is.append(img.permute(1, 2, 0).numpy())\n",
        "            \n",
        "            # extract and append the ground truth boxes and labels\n",
        "            boxes_gt    = target['boxes'].cpu().numpy()\n",
        "            labels_gt   = target['labels'].cpu().numpy()\n",
        "            boxes_gts.append(boxes_gt)\n",
        "            labels_gts.append(labels_gt)\n",
        "\n",
        "            # extract the predicted boxes, labels, and scores\n",
        "            boxes_pred  = prediction['boxes']\n",
        "            labels_pred = prediction['labels']\n",
        "            scores_pred = prediction['scores']\n",
        "\n",
        "            # apply non maximum suppression (IoU_threshold=0.2 corresponds with 33 procent overlap for two equally sized squares)\n",
        "            boxes_nms, labels_nms, scores_nms = do_nms(boxes_pred, labels_pred, scores_pred, IoU_threshold=0.2)\n",
        "\n",
        "            # append the predicted boxes, labels, and scores that follow from the non maximum suppression\n",
        "            boxes_preds.append(boxes_nms.cpu().numpy())\n",
        "            labels_preds.append(labels_nms.cpu().numpy())\n",
        "            scores_preds.append(scores_nms.cpu().numpy())\n",
        "\n",
        "    return Is, boxes_gts, labels_gts, boxes_preds, labels_preds, scores_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9toI7Pcu49TT",
        "scrolled": false
      },
      "source": [
        "def plot_ground_truth_and_prediction(I, boxes_gt, labels_gt, boxes_pred, labels_pred, scores_pred, decision_threshold):\n",
        "    boxes_pred  = boxes_pred[scores_pred>decision_threshold]\n",
        "    labels_pred = labels_pred[scores_pred>decision_threshold]\n",
        "\n",
        "    figure, ax = plt.subplots(1, 2, figsize=(8, 8))\n",
        "    figure.tight_layout()\n",
        "    figure.set_figwidth(13)\n",
        "\n",
        "    ax[0].imshow(I, zorder=-10)\n",
        "    for i in range(len(boxes_gt)):\n",
        "        bbox = boxes_gt[i]\n",
        "        rectangle_points = np.array([[bbox[0], bbox[2], bbox[2], bbox[0], bbox[0]],\n",
        "                                      [bbox[1], bbox[1], bbox[3], bbox[3], bbox[1]]])\n",
        "        label = labels_gt[i]\n",
        "        if label==1:\n",
        "            ax[0].plot(rectangle_points[0,:], rectangle_points[1,:], color='blue', linewidth=3, zorder=10)\n",
        "        elif label==2:\n",
        "            ax[0].plot(rectangle_points[0,:], rectangle_points[1,:], color='red', linewidth=3, zorder=20) \n",
        "    ax[0].set_title('Ground truth', fontsize=24)  \n",
        "    ax[0].set_xlabel('Width of image (pixels)', fontsize=20)\n",
        "    ax[0].set_ylabel('Height of image (pixels)', fontsize=20)\n",
        "    ax[0].set_xticks(np.linspace(0, 900, 7))\n",
        "    ax[0].set_yticks(np.linspace(0, 1200, 9))\n",
        "    ax[0].tick_params(axis='both', which='major', labelsize=16)\n",
        "    ax[0].set_xlim(0, 900)\n",
        "    ax[0].set_ylim(1200, 0)\n",
        "\n",
        "    ax[1].imshow(I, zorder=-10)\n",
        "    for i in range(len(boxes_pred)):\n",
        "        bbox = boxes_pred[i]\n",
        "        rectangle_points = np.array([[bbox[0], bbox[2], bbox[2], bbox[0], bbox[0]],\n",
        "                                      [bbox[1], bbox[1], bbox[3], bbox[3], bbox[1]]])\n",
        "        label = labels_pred[i]\n",
        "        if label==1:\n",
        "            ax[1].plot(rectangle_points[0,:], rectangle_points[1,:], color='blue', linewidth=3, zorder=10)\n",
        "        elif label==2:\n",
        "            ax[1].plot(rectangle_points[0,:], rectangle_points[1,:], color='red', linewidth=3, zorder=20)\n",
        "    ax[1].set_title('Predicted', fontsize=24)\n",
        "    ax[1].set_xlabel('Width of image (pixels)', fontsize=20)\n",
        "    ax[1].set_ylabel('Height of image (pixels)', fontsize=20)\n",
        "    ax[1].set_xticks(np.linspace(0, 900, 7))\n",
        "    ax[1].set_yticks(np.linspace(0, 1200, 9))\n",
        "    ax[1].tick_params(axis='both', which='major', labelsize=16)\n",
        "    ax[1].set_xlim(0, 900)\n",
        "    ax[1].set_ylim(1200, 0)\n",
        "\n",
        "    line_raw  = Line2D([0], [0], color='blue', linewidth=3, label='raw')\n",
        "    line_ripe = Line2D([0], [0], color='red', linewidth=3, label='ripe')\n",
        "    plt.legend(handles=[line_raw, line_ripe], loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20)    \n",
        "\n",
        "    for axis in ax.flat:\n",
        "        axis.label_outer()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlUJvfZD0j-y"
      },
      "source": [
        "if DO_TEST:\n",
        "    Is, boxes_gts, labels_gts, boxes_preds, labels_preds, scores_preds = test_model(model, device, dataset_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuHqetIsABWi"
      },
      "source": [
        "if DO_TEST:\n",
        "    ipywidgets.interact(lambda idx, threshold: plot_ground_truth_and_prediction(Is[idx],\n",
        "                                                                     boxes_gts[idx], \n",
        "                                                                     labels_gts[idx], \n",
        "                                                                     boxes_preds[idx], \n",
        "                                                                     labels_preds[idx], \n",
        "                                                                     scores_preds[idx],\n",
        "                                                                     decision_threshold=threshold), \n",
        "                        idx=range(len(Is)),\n",
        "                        threshold=ipywidgets.FloatSlider(min=0, max=1, step=0.05))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyJAga1rG8Dk"
      },
      "source": [
        "## 10. Optimize decision threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4blb9UgdQfU"
      },
      "source": [
        "def classify_boxes(boxes_ripe_gt, boxes_ripe_pred, IoU_threshold=0.5):\n",
        "    dataframe_columns = ['x_min', 'y_min', 'x_max', 'y_max', 'idx_gt', 'idx_pred', 'type']\n",
        "    boxes_result = pd.DataFrame(np.empty([0,7]), columns=dataframe_columns)\n",
        "\n",
        "    if len(boxes_ripe_pred)>0 and len(boxes_ripe_gt)>0:\n",
        "        # array with the IoU values, row is [IoU_box_gt1, IoU_box_gt2, ...]\n",
        "        iou_values = np.empty([len(boxes_ripe_pred), len(boxes_ripe_gt)])\n",
        "\n",
        "        for i, coor_pred in enumerate(boxes_ripe_pred):\n",
        "            x_min, y_min, x_max, y_max = coor_pred\n",
        "            for j, coor_gt in enumerate(boxes_ripe_gt):\n",
        "                x_min_gt, y_min_gt, x_max_gt, y_max_gt = coor_gt\n",
        "\n",
        "                x1_intersect = max(x_min, x_min_gt)\n",
        "                x2_intersect = min(x_max, x_max_gt)\n",
        "                y1_intersect = max(y_min, y_min_gt)\n",
        "                y2_intersect = min(y_max, y_max_gt)\n",
        "\n",
        "                if x1_intersect<x2_intersect and y1_intersect<y2_intersect:\n",
        "                    area_pred       = (x_max-x_min)*(y_max-y_min)\n",
        "                    area_gt         = (x_max_gt-x_min_gt)*(y_max_gt-y_min_gt)\n",
        "                    area_intersect  = (x2_intersect-x1_intersect)*(y2_intersect-y1_intersect) \n",
        "                    iou_values[i,j] = area_intersect/(area_pred+area_gt-area_intersect) \n",
        "                else:\n",
        "                    iou_values[i,j] = 0\n",
        "\n",
        "        iou_values_argmax0 = np.argmax(iou_values, axis=0)\n",
        "        iou_values_argmax1 = np.argmax(iou_values, axis=1)\n",
        "\n",
        "        for idx in range(len(boxes_ripe_gt)):\n",
        "            # get TP\n",
        "            if iou_values_argmax1[iou_values_argmax0[idx]]==idx and iou_values[iou_values_argmax0[idx],idx]>IoU_threshold:\n",
        "                new_box = pd.DataFrame(np.empty([1,7]), columns=dataframe_columns).astype(object)\n",
        "                new_box.loc[0,['x_min','y_min','x_max','y_max']] = boxes_ripe_pred[iou_values_argmax0[idx],:]\n",
        "                new_box.at[0,'idx_gt']    = idx\n",
        "                new_box.at[0,'idx_pred']  = iou_values_argmax0[idx]\n",
        "                new_box.at[0,'type']      = 'TP'\n",
        "                boxes_result = boxes_result.append(new_box)\n",
        "            # get FN\n",
        "            else:\n",
        "                new_box = pd.DataFrame(np.empty([1,7]), columns=dataframe_columns).astype(object)\n",
        "                new_box.loc[0,['x_min','y_min','x_max','y_max']] = boxes_ripe_gt[idx,:]\n",
        "                new_box.at[0,'idx_gt']    = idx\n",
        "                new_box.at[0,'idx_pred']  = None\n",
        "                new_box.at[0,'type']      = 'FN'\n",
        "                boxes_result = boxes_result.append(new_box)\n",
        "\n",
        "        TP_ids = [idx for idx in boxes_result['idx_pred'].tolist() if idx is not None]\n",
        "        FP_ids = [idx for idx in range(len(boxes_ripe_pred)) if idx not in TP_ids]\n",
        "\n",
        "        # get FP\n",
        "        for idx in FP_ids:\n",
        "            new_box = pd.DataFrame(np.empty([1,7]), columns=dataframe_columns).astype(object)\n",
        "            new_box.loc[0,['x_min','y_min','x_max','y_max']] = boxes_ripe_pred[idx,:]\n",
        "            new_box.at[0,'idx_gt']    = None\n",
        "            new_box.at[0,'idx_pred']  = idx\n",
        "            new_box.at[0,'type']      = 'FP'\n",
        "            boxes_result = boxes_result.append(new_box)\n",
        "\n",
        "        boxes_result = boxes_result.sort_values(['type'])\n",
        "        boxes_result.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    elif len(boxes_ripe_pred)>0 and len(boxes_ripe_gt)==0:\n",
        "        # get FP\n",
        "        for idx in range(len(boxes_ripe_pred)):\n",
        "            new_box = pd.DataFrame(np.empty([1,7]), columns=dataframe_columns).astype(object)\n",
        "            new_box.loc[0,['x_min','y_min','x_max','y_max']] = boxes_ripe_pred[idx,:]\n",
        "            new_box.at[0,'idx_gt']    = None\n",
        "            new_box.at[0,'idx_pred']  = idx\n",
        "            new_box.at[0,'type']      = 'FP'\n",
        "            boxes_result = boxes_result.append(new_box)\n",
        "\n",
        "        boxes_result = boxes_result.sort_values(['type'])\n",
        "        boxes_result.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    elif len(boxes_ripe_pred)==0 and len(boxes_ripe_gt)>0:\n",
        "        # get FN\n",
        "        for idx in range(len(boxes_ripe_gt)):\n",
        "            new_box = pd.DataFrame(np.empty([1,7]), columns=dataframe_columns).astype(object)\n",
        "            new_box.loc[0,['x_min','y_min','x_max','y_max']] = boxes_ripe_gt[idx,:]\n",
        "            new_box.at[0,'idx_gt']    = idx\n",
        "            new_box.at[0,'idx_pred']  = None\n",
        "            new_box.at[0,'type']      = 'FN'\n",
        "            boxes_result = boxes_result.append(new_box)\n",
        "\n",
        "        boxes_result = boxes_result.sort_values(['type'])\n",
        "        boxes_result.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return boxes_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRatF3tk2azN"
      },
      "source": [
        "def calculate_boxes_results(Is, boxes_gts, labels_gts, boxes_preds, labels_preds, scores_preds, IoU_threshold, decision_threshold):\n",
        "    boxes_results = []\n",
        "    for i in range(len(Is)):\n",
        "        boxes_gt    = boxes_gts[i]\n",
        "        labels_gt   = labels_gts[i]\n",
        "        scores_pred = scores_preds[i]\n",
        "        boxes_pred  = boxes_preds[i][scores_pred>decision_threshold]\n",
        "        labels_pred = labels_preds[i][scores_pred>decision_threshold]\n",
        "\n",
        "        boxes_ripe_gt   = boxes_gt[labels_gt==2]\n",
        "        boxes_ripe_pred = boxes_pred[labels_pred==2]\n",
        "\n",
        "        boxes_result = classify_boxes(boxes_ripe_gt, boxes_ripe_pred, IoU_threshold)\n",
        "        boxes_results.append(boxes_result)\n",
        "    return boxes_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKdfEvLB-mnl"
      },
      "source": [
        "def calculate_positives_negatives(boxes_result):\n",
        "    num_TP, num_FP, num_FN = 0, 0, 0\n",
        "\n",
        "    for box_type in boxes_result['type'].tolist():\n",
        "        if box_type=='TP':\n",
        "            num_TP += 1\n",
        "        elif box_type=='FP':\n",
        "            num_FP += 1\n",
        "        elif box_type=='FN':\n",
        "            num_FN += 1\n",
        "\n",
        "    return num_TP, num_FP, num_FN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DxzZsmk-o1N"
      },
      "source": [
        "def calculate_f1_score(num_TP, num_FP, num_FN):\n",
        "    if num_TP+num_FP>0 and num_TP+num_FN>0 and num_TP>0:\n",
        "        precision   = num_TP/(num_TP+num_FP)\n",
        "        recall      = num_TP/(num_TP+num_FN)\n",
        "        f1_score    = 2*precision*recall/(precision+recall)\n",
        "    else:\n",
        "        precision   = np.nan\n",
        "        recall      = np.nan\n",
        "        f1_score    = np.nan\n",
        "    return f1_score, precision, recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJuwNkngHZSR"
      },
      "source": [
        "def calculate_optimal_decision_threshold(model, device, dataset):\n",
        "    decision_thresholds   = np.linspace(0, 1, 101).tolist()\n",
        "    f1_scores             = []\n",
        "\n",
        "    # predictions for dataset\n",
        "    Is, boxes_gts, labels_gts, boxes_preds, labels_preds, scores_preds = test_model(model, device, dataset)\n",
        "\n",
        "    # calculate f1 score for every decision threshold\n",
        "    for threshold in decision_thresholds:\n",
        "        # boxes_results\n",
        "        boxes_results = calculate_boxes_results(Is, boxes_gts, labels_gts, boxes_preds, labels_preds, scores_preds, IoU_threshold=0.5, decision_threshold=threshold)\n",
        "\n",
        "        # TP, FP, and FN\n",
        "        total_num_TP, total_num_FP, total_num_FN = 0, 0, 0\n",
        "        for boxes_result in boxes_results:\n",
        "            num_TP, num_FP, num_FN = calculate_positives_negatives(boxes_result)\n",
        "            total_num_TP += num_TP\n",
        "            total_num_FP += num_FP\n",
        "            total_num_FN += num_FN\n",
        "\n",
        "        # results\n",
        "        f1_score, precision, recall = calculate_f1_score(total_num_TP, total_num_FP, total_num_FN)\n",
        "        f1_scores.append(f1_score)\n",
        "\n",
        "    # calculate optimal value\n",
        "    decision_thresholds   = np.array(decision_thresholds)[~np.isnan(f1_scores)]\n",
        "    f1_scores             = np.array(f1_scores)[~np.isnan(f1_scores)]\n",
        "    decision_threshold    = decision_thresholds[np.argmax(f1_scores)]\n",
        "    f1_score              = np.max(f1_scores)\n",
        "\n",
        "    return decision_thresholds, f1_scores, decision_threshold, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRGxO_xHZXG"
      },
      "source": [
        "if DO_TEST:\n",
        "    if DO_OPTIMIZE_THRESHOLD:\n",
        "        decision_thresholds, f1_scores, decision_threshold, f1_score = calculate_optimal_decision_threshold(model, device, dataset_val)\n",
        "        print('The optimal decision threshold is {}'.format(decision_threshold))\n",
        "    else:\n",
        "        decision_threshold = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5otuskzJkNN"
      },
      "source": [
        "if DO_TEST and DO_OPTIMIZE_THRESHOLD:\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.plot(decision_thresholds, f1_scores, color='blue', linewidth=3, zorder=10)\n",
        "    plt.scatter(decision_threshold, f1_score, s=100, color='red', zorder=20, label='Optimal decision threshold')\n",
        "    plt.xlabel('Decision threshold', fontsize=20)\n",
        "    plt.ylabel('F1 score', fontsize=20)\n",
        "    plt.xticks(np.linspace(0, 1, 11), fontsize=16)\n",
        "    plt.yticks(np.linspace(0, min(int(10*(max(f1_scores)+0.1))/10, 1), int(10*min(int(10*(max(f1_scores)+0.1))/10, 1)+1)), fontsize=16)\n",
        "    plt.xlim(0, 1)\n",
        "    plt.ylim(0, min(int(10*(max(f1_scores)+0.1))/10, 1))\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20) \n",
        "    plt.grid(zorder=-10)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESmWborkHZ9c"
      },
      "source": [
        "## 11. Calculate results and show results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oBKriaU1GCi"
      },
      "source": [
        "def plot_img_and_results(I, boxes_result):\n",
        "    num_TP, num_FP, num_FN = calculate_positives_negatives(boxes_result)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(I, zorder=-10)\n",
        "    for i in range(len(boxes_result)):\n",
        "        coor = boxes_result.loc[i,['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n",
        "        rectangle_points = np.array([[coor[0], coor[2], coor[2], coor[0], coor[0]],\n",
        "                                     [coor[1], coor[1], coor[3], coor[3], coor[1]]])\n",
        "        if boxes_result.loc[i,'type']=='TP':\n",
        "            plt.plot(rectangle_points[0,:], rectangle_points[1,:], color='gold', linewidth=3, zorder=10)\n",
        "        elif boxes_result.loc[i,'type']=='FP':\n",
        "            plt.plot(rectangle_points[0,:], rectangle_points[1,:], color='red', linewidth=3, zorder=10)\n",
        "        elif boxes_result.loc[i,'type']=='FN':\n",
        "            plt.plot(rectangle_points[0,:], rectangle_points[1,:], color='blue', linewidth=3, zorder=10)\n",
        "    plt.title('Result: {} TP, {} FP, {} FN'.format(num_TP, num_FP, num_FN), fontsize=24)\n",
        "    plt.xlabel('Width of image (pixels)', fontsize=20)\n",
        "    plt.ylabel('Height of image (pixels)', fontsize=20)\n",
        "    plt.xticks(np.linspace(0, 900, 7), fontsize=16)\n",
        "    plt.yticks(np.linspace(0, 1200, 9), fontsize=16)\n",
        "    plt.xlim(0, 900)\n",
        "    plt.ylim(1200, 0)\n",
        "    line_TP = Line2D([0], [0], color='gold', linewidth=3, label='TP')\n",
        "    line_FP = Line2D([0], [0], color='red', linewidth=3, label='FP')\n",
        "    line_FN = Line2D([0], [0], color='blue', linewidth=3, label='FN')\n",
        "    plt.legend(handles=[line_TP, line_FP, line_FN], loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20)\n",
        "    plt.show()\n",
        "    plt.savefig('plot1.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwnhH8L23nIh"
      },
      "source": [
        "if DO_TEST:\n",
        "    # IoU_threshold=0.5 corresponds with 67 procent overlap for two equally sized squares\n",
        "    IoU_threshold   = 0.5\n",
        "    \n",
        "    # boxes_results\n",
        "    Is, boxes_gts, labels_gts, boxes_preds, labels_preds, scores_preds = test_model(model, device, dataset_test)\n",
        "    boxes_results = calculate_boxes_results(Is, boxes_gts, labels_gts, boxes_preds, labels_preds, scores_preds, IoU_threshold, decision_threshold)\n",
        "\n",
        "    # TP, FP, and FN\n",
        "    total_num_TP, total_num_FP, total_num_FN = 0, 0, 0\n",
        "    for boxes_result in boxes_results:\n",
        "        num_TP, num_FP, num_FN = calculate_positives_negatives(boxes_result)\n",
        "        total_num_TP += num_TP\n",
        "        total_num_FP += num_FP\n",
        "        total_num_FN += num_FN\n",
        "\n",
        "    # results\n",
        "    f1_score, precision, recall = calculate_f1_score(total_num_TP, total_num_FP, total_num_FN)\n",
        "\n",
        "    print('The model has a f1_score of {:.3f} on the test dataset.'.format(f1_score))\n",
        "    print('The model has a precision of {:.3f} on the test dataset.'.format(precision))\n",
        "    print('The model has a recall of {:.3f} on the test dataset.'.format(recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWWdVwFt-5AY"
      },
      "source": [
        "if DO_TEST:\n",
        "    ipywidgets.interact(lambda idx: plot_img_and_results(Is[idx], boxes_results[idx]), idx=range(len(Is)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
